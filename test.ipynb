{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using PyTorch as our deep learning framework. \n",
    "Importing necessary libraries to pre-processing, tokenizing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the device. We will proceed if there is a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "else:\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test set and load into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"./Dataset/github-labels-top3-803k-test.csv\"):\n",
    "    !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-test.tar.gz\" | tar -xz \n",
    "    !mv github-labels-top3-803k-train.csv ./Dataset/\n",
    "\n",
    "testdf = pd.read_csv(\"./Dataset/github-labels-top3-803k-test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same label map used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'bug': 0, 'enhancement': 1, 'question': 2}\n",
    "testdf['label'] = testdf.issue_label.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-preocessing function for removing whitespace and creating new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "    # concatenate title and body, then remove whitespaces\n",
    "    doc = \"\"\n",
    "    doc += str(row.issue_title)\n",
    "    doc += \" \"\n",
    "    doc += str(row.issue_body)\n",
    "    # https://radimrehurek.com/gensim/parsing/preprocessing.html\n",
    "    doc = gensim.parsing.preprocessing.strip_multiple_whitespaces(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying preporcessing step on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['issue_data'] = testdf.apply(preprocess, axis=1)\n",
    "\n",
    "newTestDF = testdf[['issue_label', 'issue_data', 'label']]\n",
    "testdf = newTestDF.copy()\n",
    "print(testdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating tokenizer and encoding the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    testdf.issue_data.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(testdf.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TensorDataset from encoded and masked test and creating a dataloader for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                             sampler=SequentialSampler(dataset_test),\n",
    "                             batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring function for result generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def result_generation(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "        P_c = precision_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "        R_c = recall_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "        F1_c = f1_score(labels_flat, preds_flat, average=None, labels=[label])[0]\n",
    "\n",
    "        print(f\"=*= {label_dict_inverse[label]} =*=\")\n",
    "        # print(\"Full precision:\\t\",P_c)\n",
    "        # print(\"Full recall:\\t\\t\",R_c)\n",
    "        # print(\"Full F1 score:\\t\",F1_c)\n",
    "        print(f\"precision:\\t{P_c:.4f}\")\n",
    "        print(f\"recall:\\t\\t{R_c:.4f}\")\n",
    "        print(f\"F1 score:\\t{F1_c:.4f}\")\n",
    "        print()\n",
    "\n",
    "    P = precision_score(labels_flat, preds_flat, average='micro')\n",
    "    R = recall_score(labels_flat, preds_flat, average='micro')\n",
    "    F1 = f1_score(labels_flat, preds_flat, average='micro')\n",
    "\n",
    "    print(\"=*= global =*=\")\n",
    "    print(f\"precision:\\t{P:.4f}\")\n",
    "    print(f\"recall:\\t\\t{R:.4f}\")\n",
    "    print(f\"F1 score:\\t{F1:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing seed value for random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring the function for evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader_val):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model on different model states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    print(\"Epoch: \", i)\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(\n",
    "                                                              label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(\n",
    "        './Models/finetuned_BERT_epoch_'+str(i)+'.model', map_location=device))\n",
    "\n",
    "    # %%\n",
    "    _, predictions, true_vals = evaluate(model, dataloader_test)\n",
    "\n",
    "    # %%\n",
    "    result_generation(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
